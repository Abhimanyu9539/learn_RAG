{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b37ea008",
   "metadata": {},
   "source": [
    "### Chatbot And RAG Evaluation\n",
    "\n",
    "Retrieval Augmented Generation (RAG) is a technique that enhances Large Language Models (LLMs) by providing them with relevant external knowledge. It has become one of the most widely used approaches for building LLM applications.\n",
    "\n",
    "This tutorial will show you how to evaluate your RAG applications using LangSmith. You'll learn:\n",
    "\n",
    "1. How to create test datasets\n",
    "2. How to run your RAG application on those datasets\n",
    "3. How to measure your application's performance using different evaluation metrics\n",
    "\n",
    "#### Overview\n",
    "A typical RAG evaluation workflow consists of three main steps:\n",
    "\n",
    "1. Creating a dataset with questions and their expected answers\n",
    "2. Running your RAG application on those questions\n",
    "3. Using evaluators to measure how well your application performed, looking at factors like:\n",
    " - Answer relevance\n",
    " - Answer accuracy\n",
    " - Retrieval quality\n",
    " \n",
    "For this tutorial, we'll create and evaluate a bot that answers questions about a few of Lilian Weng's insightful blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f0645f",
   "metadata": {},
   "source": [
    "### Chatbot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c911dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6136fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_ids': ['a3ca34ed-9916-4888-bb3d-10e48944a778',\n",
       "  'c2b8dd13-37d5-4fc0-aa1b-bc8f7532c294',\n",
       "  'febc493e-76d2-458a-9ac7-24b25f52589a',\n",
       "  '5ab95691-7741-436d-995b-dcc50501c110',\n",
       "  'bebd16d3-1284-43f1-94a0-1322b17c88cb'],\n",
       " 'count': 5}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Define dataset: these are your test cases\n",
    "dataset_name = \"Chatbots Evaluation 2\"\n",
    "dataset = client.create_dataset(dataset_name)\n",
    "client.create_examples(\n",
    "    dataset_id=dataset.id,\n",
    "    examples=[\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangChain?\"},\n",
    "            \"outputs\": {\"answer\": \"A framework for building LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is LangSmith?\"},\n",
    "            \"outputs\": {\"answer\": \"A platform for observing and evaluating LLM applications\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is OpenAI?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Google?\"},\n",
    "            \"outputs\": {\"answer\": \"A technology company known for search\"},\n",
    "        },\n",
    "        {\n",
    "            \"inputs\": {\"question\": \"What is Mistral?\"},\n",
    "            \"outputs\": {\"answer\": \"A company that creates Large Language Models\"},\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a8f49",
   "metadata": {},
   "source": [
    "### Define Metrics (LLM AS A JUDGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cc6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith import wrappers\n",
    "\n",
    "openai_client = wrappers.wrap_openai(openai.OpenAI())\n",
    "eval_instructions = \"You are an expert professor specialized in grading students' answers to questions.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec0470d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith import wrappers\n",
    " \n",
    "openai_client=wrappers.wrap_openai(openai.OpenAI())\n",
    "\n",
    "eval_instructions = \"You are an expert professor specialized in grading students' answers to questions.\"\n",
    "\n",
    "def correctness(inputs:dict,outputs:dict, reference_outputs:dict)->bool:\n",
    "      user_content = f\"\"\"You are grading the following question:\n",
    "    {inputs['question']}\n",
    "    Here is the real answer:\n",
    "    {reference_outputs['answer']}\n",
    "    You are grading the following predicted answer:\n",
    "    {outputs['response']}\n",
    "    Respond with CORRECT or INCORRECT:\n",
    "    Grade:\n",
    "    \"\"\"\n",
    "      response=openai_client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                  {\"role\":\"system\",\"content\":eval_instructions},\n",
    "                  {\"role\":\"user\",\"content\":user_content}\n",
    "            ]\n",
    "      ).choices[0].message.content\n",
    "\n",
    "      return response == \"CORRECT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5c6eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concisions- checks whether the actual output is less than 2x the length of the expected result.\n",
    "\n",
    "def concision(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    return int(len(outputs[\"response\"]) < 2 * len(reference_outputs[\"answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201c0f6a",
   "metadata": {},
   "source": [
    "### Run Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ad6fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_instructions = \"Respond to the users question in a short, concise manner (one short sentence).\"\n",
    "def my_app(question: str, model: str = \"gpt-4o-mini\", instructions: str = default_instructions) -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": instructions},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "    ).choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f0a8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call my_app for each question\n",
    "\n",
    "def ls_target(inputs:str) -> dict:\n",
    "    return {\"response\": my_app(inputs[\"question\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cd08b84c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'gpt-4o-mini-b4912e4d' at:\n",
      "https://smith.langchain.com/o/888508c2-6024-4c31-b81c-8eca3c339169/datasets/24c29ada-bd25-4343-b77e-bc80644635eb/compare?selectedSessions=92282181-6300-48fb-b5e8-fbfbbe2ae395\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:08,  1.69s/it]\n"
     ]
    }
   ],
   "source": [
    "## Run Evaluations\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    ls_target,  ## You AI system\n",
    "    data= dataset_name, \n",
    "    evaluators = [correctness, concision], \n",
    "    experiment_prefix = \"gpt-4o-mini\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "816efa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Call my_app for each question\n",
    "\n",
    "def ls_target(inputs:str) -> dict:\n",
    "    return {\"response\": my_app(inputs[\"question\"], model=\"gpt-4o\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134c3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run Evaluations\n",
    "\n",
    "experiment_results = client.evaluate(\n",
    "    ls_target,  ## You AI system\n",
    "    data= dataset_name, \n",
    "    evaluators = [correctness, concision], \n",
    "    experiment_prefix = \"gpt-4o\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d316f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeb188c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
