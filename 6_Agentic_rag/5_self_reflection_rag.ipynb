{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6aff18",
   "metadata": {},
   "source": [
    "### üß† What is Self-Reflection in RAG?\n",
    "Self-reflection = LLM evaluates its own output:\n",
    "‚ÄúIs this clear, complete, and accurate?‚Äù\n",
    "\n",
    "#### Self-Reflection in RAG using LangGraph, we‚Äôll design a workflow where the agent:\n",
    "\n",
    "1. Generates an initial answer using retrieved context\n",
    "2. Reflects on that answer with a dedicated self-critic LLM step\n",
    "3. If unsatisfied, it can revise the query, retrieve again, or regenerate the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10477753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08713f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001BB59FCA900>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001BB59FCB380>, root_client=<openai.OpenAI object at 0x000001BB59FC8050>, root_async_client=<openai.AsyncOpenAI object at 0x000001BB59FCB0E0>, model_name='gpt-5-nano', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load LLM models\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "llm= init_chat_model(\"openai:gpt-5-nano\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2faaecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"research_notes.txt\", encoding=\"utf-8\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a59a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. State Definition\n",
    "\n",
    "class RAGReflectionState(BaseModel):\n",
    "    question : str\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = ''\n",
    "    reflection : str = ''\n",
    "    revised : bool = False\n",
    "    attempts : int = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19e679b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Nodes\n",
    "\n",
    "# a. Retrive\n",
    "def retrieve_docs(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    docs = retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"retrieved_docs\": docs})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5d222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Generate Answer\n",
    "\n",
    "def generate_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "    Use the following context to answer the question\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {state.question}\n",
    "    \"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"answer\": answer, \"attempts\": state.attempts + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d790d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Self-Reflect\n",
    "def reflect_on_answer(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "        Reflect on the following answer to see if it fully addresses the question. \n",
    "        State YES if it is complete and correct, or NO with an explanation.\n",
    "\n",
    "        Question: {state.question}\n",
    "\n",
    "        Answer: {state.answer}\n",
    "\n",
    "        Respond like:\n",
    "        Reflection: YES or NO\n",
    "        Explanation: ...\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(prompt).content\n",
    "    is_ok = \"reflection: yes\" in result.lower()\n",
    "    return state.model_copy(update={\"reflection\": result, \"revised\": not is_ok})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4c7731c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Finalizer\n",
    "def finalize(state: RAGReflectionState) -> RAGReflectionState:\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de11353c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Langgraph DAG\n",
    "\n",
    "builder = StateGraph(RAGReflectionState)\n",
    "\n",
    "builder.add_node(\"retriever\", retrieve_docs)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "builder.add_node(\"reflector\", reflect_on_answer)\n",
    "builder.add_node(\"done\", finalize)\n",
    "\n",
    "builder.set_entry_point(\"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\", \n",
    "    lambda s: \"done\" if not s.revised or s.attempts >= 2 else \"retriever\"\n",
    ")\n",
    "builder.add_edge(\"done\", END)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6123c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b15317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Final Answer:\n",
      " - EfficientFormer ‚Äî deployed on Raspberry Pi 4 (quantized int8 mode works with minimal accuracy loss)\n",
      "- TinyBERT ‚Äî used for production classification (support ticket priority tagging)\n",
      "- LLaMA2 with FlashAttention2 ‚Äî integrated to reduce context latency (‚âà50% reduction)\n",
      "\n",
      "These variants are indicated as active in deployment contexts within the log.\n",
      "\n",
      "üîÅ Reflection Log:\n",
      " Reflection: YES\n",
      "Explanation: The answer lists three transformers variants used in production deployments (EfficientFormer on Raspberry Pi 4 with int8 quantization, TinyBERT for production classification, and LLaMA2 with FlashAttention2 to cut context latency). It also notes these are active in deployment logs. If there are additional variants or details, they may not be captured in the current snippet.\n",
      "üîÑ Total Attempts: 1\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# 5. Run the Agent\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"What are the transformers variants in production deployments?\"\n",
    "    init_state = RAGReflectionState(question=user_query)\n",
    "    result = graph.invoke(init_state)\n",
    "\n",
    "    print(\"\\nüß† Final Answer:\\n\", result[\"answer\"])\n",
    "    print(\"\\nüîÅ Reflection Log:\\n\", result[\"reflection\"])\n",
    "    print(\"üîÑ Total Attempts:\", result[\"attempts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
