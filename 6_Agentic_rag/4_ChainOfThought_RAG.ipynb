{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9f3ee0",
   "metadata": {},
   "source": [
    "#### Chain Of Thoughts With RAG\n",
    "What is Chain-of-Thought (CoT) in RAG?\n",
    "\n",
    "CoT reasoning breaks down a complex question into intermediate steps, and allows retrieval + reflection at each step before answering.\n",
    "\n",
    "User Query\n",
    "   ↓\n",
    "- Step 1: Decompose question → sub-steps (Reason)\n",
    "- Step 2: Retrieve docs per step (Act)\n",
    "- Step 3: Combine context (Observe)\n",
    "- Step 4: Final answer generation (Reflect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f42502",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5eb203",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Prepare Vector store\n",
    "\n",
    "docs = TextLoader(\"research_notes.txt\",encoding=\"utf-8\").load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500, \n",
    "    chunk_overlap = 50\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d030d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f08afd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Langgraph State Definition\n",
    "\n",
    "class RAGCoTState(BaseModel):\n",
    "    question: str\n",
    "    sub_steps: list[str] = []\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer:str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d24e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Nodes\n",
    "\n",
    "## a. Plan Sub Question\n",
    "\n",
    "def plan_steps(state: RAGCoTState) -> RAGCoTState:\n",
    "    prompt = f\"Break the question into 2-3 reasoning steps: \\n\\n {state.question}\"\n",
    "    result = llm.invoke(prompt).content\n",
    "    subqs = [line.strip(\"- \")for line in result.split(\"\\n\") if line.strip()]\n",
    "\n",
    "    return state.model_copy(update={\"sub_steps\": subqs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdc3b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "## b. Retrievr for each step\n",
    "\n",
    "def retrieve_per_step(state: RAGCoTState) -> RAGCoTState:\n",
    "    all_docs = []\n",
    "    for sub in state.sub_steps:\n",
    "        docs = retriever.invoke(sub)\n",
    "        all_docs.extend(docs)\n",
    "    \n",
    "    return state.model_copy(update={\"retrieved_docs\": all_docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a73347",
   "metadata": {},
   "outputs": [],
   "source": [
    "## c. Generate Final Answer\n",
    "\n",
    "def generate_answer(state:RAGCoTState) -> RAGCoTState:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "        You are answering a complex question using reasoning and retrieved documents.\n",
    "\n",
    "        Question: {state.question}\n",
    "\n",
    "        Relevant Information:\n",
    "        {context}\n",
    "\n",
    "        Now synthesize a well-reasoned final answer.\n",
    "        \"\"\"\n",
    "    result = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update = {\"answer\": result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b7d0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4. Langgraph Graph\n",
    "\n",
    "builder = StateGraph(RAGCoTState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_steps)\n",
    "builder.add_node(\"retriever\", retrieve_per_step)\n",
    "builder.add_node(\"responder\", generate_answer)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62c98c13",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 400.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG\\learn_RAG\\.venv\\Lib\\site-packages\\IPython\\core\\formatters.py:1036\u001b[39m, in \u001b[36mMimeBundleFormatter.__call__\u001b[39m\u001b[34m(self, obj, include, exclude)\u001b[39m\n\u001b[32m   1033\u001b[39m     method = get_real_method(obj, \u001b[38;5;28mself\u001b[39m.print_method)\n\u001b[32m   1035\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG\\learn_RAG\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:758\u001b[39m, in \u001b[36mPregel._repr_mimebundle_\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    754\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_repr_mimebundle_\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    755\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Mime bundle used by Jupyter to display the graph\"\"\"\u001b[39;00m\n\u001b[32m    756\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    757\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext/plain\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m),\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mimage/png\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    759\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG\\learn_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph.py:695\u001b[39m, in \u001b[36mGraph.draw_mermaid_png\u001b[39m\u001b[34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[39m\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunnables\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_mermaid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m draw_mermaid_png\n\u001b[32m    689\u001b[39m mermaid_syntax = \u001b[38;5;28mself\u001b[39m.draw_mermaid(\n\u001b[32m    690\u001b[39m     curve_style=curve_style,\n\u001b[32m    691\u001b[39m     node_colors=node_colors,\n\u001b[32m    692\u001b[39m     wrap_label_n_words=wrap_label_n_words,\n\u001b[32m    693\u001b[39m     frontmatter_config=frontmatter_config,\n\u001b[32m    694\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG\\learn_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:294\u001b[39m, in \u001b[36mdraw_mermaid_png\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[39m\n\u001b[32m    288\u001b[39m     img_bytes = asyncio.run(\n\u001b[32m    289\u001b[39m         _render_mermaid_using_pyppeteer(\n\u001b[32m    290\u001b[39m             mermaid_syntax, output_file_path, background_color, padding\n\u001b[32m    291\u001b[39m         )\n\u001b[32m    292\u001b[39m     )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m draw_method == MermaidDrawMethod.API:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     img_bytes = \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    302\u001b[39m     supported_methods = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join([m.value \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m MermaidDrawMethod])\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\RAG\\learn_RAG\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:451\u001b[39m, in \u001b[36m_render_mermaid_using_api\u001b[39m\u001b[34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[39m\n\u001b[32m    446\u001b[39m     \u001b[38;5;66;03m# For other status codes, fail immediately\u001b[39;00m\n\u001b[32m    447\u001b[39m     msg = (\n\u001b[32m    448\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m     ) + error_msg_suffix\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (requests.RequestException, requests.Timeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attempt < max_retries:\n\u001b[32m    455\u001b[39m         \u001b[38;5;66;03m# Exponential backoff with jitter\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 400.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph at 0x1ecf06348a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c24ecc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🪜 Reasoning Steps: ['To address the question of what additional experiments are involved in evaluating Transformers, we can break it down into the following reasoning steps:', '1. **Identify Core Evaluation Metrics and Benchmarks:**', 'Begin by understanding the standard evaluation metrics typically used for Transformers, such as accuracy, F1 score, BLEU score (for translation tasks), etc.', 'Consider the benchmark datasets typically used to evaluate Transformer models in different tasks like NLP, vision, etc., for example, GLUE, SuperGLUE, ImageNet, etc.', '2. **Determine Additional Experimental Dimensions:**', 'Assess the need for additional evaluation dimensions such as robustness (adversarial testing, noise), model interpretability, and efficiency (inference time, computational resource usage).', 'Consider experiments focusing on ethical aspects like bias detection, fairness, and the impact of model compressions or optimizations on performance.', '3. **Explore Advanced Evaluation Techniques or Scenarios:**', 'Evaluate Transformers in zero-shot or few-shot scenarios to gauge their ability to generalize with limited data.', 'Investigate the performance of Transformers in multi-tasking or transfer learning settings.', 'Perform ablation studies to understand the contribution of different model components or settings, such as hyperparameter variations, architecture modifications, or training regimes.', 'These steps provide a structured approach to identify and implement additional experiments necessary for a comprehensive evaluation of Transformer models beyond standard practices.']\n",
      "\n",
      "✅ Final Answer:\n",
      " To address the question about the additional experiments in Transformer evaluation, we can synthesize information from the retrieved document. The experiments are diverse, reflecting advanced techniques and practical considerations in improving transformer models.\n",
      "\n",
      "1. **FlashAttention2**: This experiment integrates FlashAttention2 into LLaMA2, resulting in a substantial reduction in context latency by about 50%. This suggests an improvement in processing efficiency, particularly relevant in scenarios needing rapid response times.\n",
      "\n",
      "2. **Chain-of-Thought Prompting**: This experiment demonstrates superior performance over direct answer prompting, particularly in logic tasks, with an 8% improvement in accuracy. Additionally, reflective prompting was found to increase accuracy by 3%, indicating advancements in how transformer models can reason through a series of logical steps.\n",
      "\n",
      "3. **Tool-Augmented Prompting**: This approach involves using LangGraph, Wikipedia, and SQL search to facilitate dynamic retrieval-agent reasoning. This experiment highlights the enhanced ability of models to integrate and cross-reference external data sources dynamically for better decision-making and retrieval accuracy.\n",
      "\n",
      "4. **Human Evaluation Protocol**: Incorporating both internal annotators and GPT-4 as synthetic evaluators for scoring fluency, helpfulness, and correctness, this experiment aims to ensure qualitative assessment in transformer evaluations, balancing human judgment with AI consistency.\n",
      "\n",
      "5. **Retrieval Experiments**: These experiments tested a hybrid dense and sparse retriever approach. The experiments compared Weaviate with FAISS plus BM25 reranking, finding that while FAISS is more efficient, Weaviate showed helpful integration capabilities with GraphQL for filtering, balancing efficiency with recall in retrieval systems.\n",
      "\n",
      "6. **LoRA Tuning**: Light-weight and Rank-tuning (LoRA) is utilized here for adapter-based fine-tuning, focusing on reduction in GPU memory use (by 60%) while maintaining performance, showing the emphasis on efficient model tuning practices.\n",
      "\n",
      "7. **Safety Module**: This includes using Detoxify for toxicity detection, employing a zero-shot classifier for out-of-scope filter mechanisms, and engaging in red teaming for adversarial prompt testing. These experiments highlight a comprehensive approach to ensuring transformer safety in deployment, addressing ethical and practical challenges.\n",
      "\n",
      "Together, these experiments depict a rigorous evaluation of transformers, exploring latency improvements, logical reasoning, dynamic retrieval, qualitative assessment, tuning efficiency, and safety, marking a comprehensive effort to advance transformer applications.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 5. Run CoT RAG Agent\n",
    "# -------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"what are the additional eperiments in Transformer eveluation?\"\n",
    "    state = RAGCoTState(question=query)\n",
    "    final = graph.invoke(state)\n",
    "\n",
    "    print(\"\\n🪜 Reasoning Steps:\", final[\"sub_steps\"])\n",
    "    print(\"\\n✅ Final Answer:\\n\", final[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665f950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
