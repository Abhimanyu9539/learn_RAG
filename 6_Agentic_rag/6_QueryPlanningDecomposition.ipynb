{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fcdb54",
   "metadata": {},
   "source": [
    "### üß† What is Query Planning and Decomposition?\n",
    "Query Planning and Decomposition is a technique where a complex user query is broken down into simpler sub-questions or tasks, allowing a system (like a RAG agent) to:\n",
    "\n",
    "- Understand the question more deeply\n",
    "- Retrieve more precise and complete information\n",
    "- Execute step-by-step reasoning\n",
    "\n",
    "It's like reverse-engineering a question into manageable steps before answering.\n",
    "\n",
    "üß† What's New in This Version?\n",
    "- ‚úÖ Add a Query Planner Node\n",
    "- ‚úÖ Break complex user queries into sub-questions\n",
    "- ‚úÖ Retrieve docs per sub-question\n",
    "- ‚úÖ Combine all retrieved contexts\n",
    "- ‚úÖ Generate a final consolidated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc169d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader,WebBaseLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ec6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ed9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Load and Embed Documents\n",
    "# ----------------------------\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(WebBaseLoader(url).load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a2f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. State Schema\n",
    "# ----------------------------\n",
    "class RAGState(BaseModel):\n",
    "    question: str\n",
    "    sub_questions: List[str] = []\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d4f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Nodes\n",
    "# ----------------------------\n",
    "\n",
    "## a. Query Planner: splits input question\n",
    "def plan_query(state: RAGState) -> RAGState:\n",
    "   \n",
    "    prompt = f\"\"\"\n",
    "        Break the following complex question into 2-3 sub-questions:\n",
    "\n",
    "        Question: {state.question}\n",
    "\n",
    "        Sub-questions:\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt)\n",
    "    sub_questions = [line.strip(\"- \").strip() for line in result.content.strip().split(\"\\n\") if line.strip()]\n",
    "    return RAGState(question=state.question, sub_questions=sub_questions)\n",
    "\n",
    "## b. Retrieve documents for each sub-question\n",
    "def retrieve_for_each(state: RAGState) -> RAGState:\n",
    "    all_docs = []\n",
    "    for sub in state.sub_questions:\n",
    "        docs = retriever.invoke(sub)\n",
    "        all_docs.extend(docs)\n",
    "    return RAGState(question=state.question, sub_questions=state.sub_questions, retrieved_docs=all_docs)\n",
    "\n",
    "## c. Generate final answer\n",
    "def generate_final_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "        Use the context below to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {state.question}\n",
    "    \"\"\"\n",
    "            \n",
    "    answer = llm.invoke(prompt).content\n",
    "    return RAGState(question=state.question, sub_questions=state.sub_questions, retrieved_docs=state.retrieved_docs, answer=answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26285e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Build LangGraph\n",
    "# ----------------------------\n",
    "builder = StateGraph(RAGState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_query)\n",
    "builder.add_node(\"retriever\", retrieve_for_each)\n",
    "builder.add_node(\"responder\", generate_final_answer)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "348521ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Explain how agent loops work and what are the challenges in diffusion video generation?', 'sub_questions': ['1. How do agent loops function in the context of diffusion video generation?', '2. What are the common challenges faced when using agent loops for generating diffusion videos?', '3. How do these challenges impact the overall process and quality of diffusion video generation?'], 'retrieved_docs': [Document(id='f3c51240-5b6d-4c63-a022-60e1fa150bd7', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='In the case of video generation, we need the diffusion model to run multiple steps of upsampling for extending video length or increasing the frame rate. This requires the capability of sampling a second video $\\\\mathbf{x}^b$ conditioned on the first $\\\\mathbf{x}^a$, $\\\\mathbf{x}^b \\\\sim p_\\\\theta(\\\\mathbf{x}^b \\\\vert \\\\mathbf{x}^a)$, where $\\\\mathbf{x}^b$ might be an autoregressive extension of $\\\\mathbf{x}^a$ or be the missing frames in-between for a video $\\\\mathbf{x}^a$ at a low frame rate.'), Document(id='d1d547c1-1f0c-4dc2-bd09-bd6f2978af5d', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Imagen Video also applies progressive distillation to speed up sampling and each distillation iteration can reduce the required sampling steps by half. Their experiments were able to distill all 7 video diffusion models down to just 8 sampling steps per model without any noticeable loss in perceptual quality.'), Document(id='02e4a446-c250-4d0c-9875-89e538671cf0', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[3] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù\\n[4] Brooks et al. ‚ÄúVideo generation models as world simulators.‚Äù OpenAI Blog, 2024.\\n[5] Zhang et al. 2023 ‚ÄúControlVideo: Training-free Controllable Text-to-Video Generation.‚Äù\\n[6] Khachatryan et al. 2023 ‚ÄúText2Video-Zero: Text-to-image diffusion models are zero-shot video generators.‚Äù\\n[7] Ho, et al. 2022 ‚ÄúImagen Video: High Definition Video Generation with Diffusion Models.‚Äù'), Document(id='048daeaa-9b4f-41a2-9c0a-ed5d24aaeca4', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.'), Document(id='c288df67-bddd-4688-9f43-3c17c78b3ecb', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent‚Äôs brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\t\\n\\tOverview of a LLM-powered autonomous agent system.\\n\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='81354a11-00dd-49a4-8e39-7a750fae35b3', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Adapting Image Models to Generate Videos\\n\\nFine-tuning on Video Data\\n\\nTraining-Free Adaptation\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:'), Document(id='f3c51240-5b6d-4c63-a022-60e1fa150bd7', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='In the case of video generation, we need the diffusion model to run multiple steps of upsampling for extending video length or increasing the frame rate. This requires the capability of sampling a second video $\\\\mathbf{x}^b$ conditioned on the first $\\\\mathbf{x}^a$, $\\\\mathbf{x}^b \\\\sim p_\\\\theta(\\\\mathbf{x}^b \\\\vert \\\\mathbf{x}^a)$, where $\\\\mathbf{x}^b$ might be an autoregressive extension of $\\\\mathbf{x}^a$ or be the missing frames in-between for a video $\\\\mathbf{x}^a$ at a low frame rate.'), Document(id='d1d547c1-1f0c-4dc2-bd09-bd6f2978af5d', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Imagen Video also applies progressive distillation to speed up sampling and each distillation iteration can reduce the required sampling steps by half. Their experiments were able to distill all 7 video diffusion models down to just 8 sampling steps per model without any noticeable loss in perceptual quality.'), Document(id='d1d547c1-1f0c-4dc2-bd09-bd6f2978af5d', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Imagen Video also applies progressive distillation to speed up sampling and each distillation iteration can reduce the required sampling steps by half. Their experiments were able to distill all 7 video diffusion models down to just 8 sampling steps per model without any noticeable loss in perceptual quality.'), Document(id='81354a11-00dd-49a4-8e39-7a750fae35b3', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='Adapting Image Models to Generate Videos\\n\\nFine-tuning on Video Data\\n\\nTraining-Free Adaptation\\n\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nDiffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:'), Document(id='618ceb74-9238-4240-a711-e6d0762aacff', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='[12] Esser et al. 2023 ‚ÄúStructure and Content-Guided Video Synthesis with Diffusion Models.‚Äù\\n[13] Bar-Tal et al. 2024 ‚ÄúLumiere: A Space-Time Diffusion Model for Video Generation.‚Äù'), Document(id='048daeaa-9b4f-41a2-9c0a-ed5d24aaeca4', metadata={'source': 'https://lilianweng.github.io/posts/2024-04-12-diffusion-video/', 'title': \"Diffusion Models for Video Generation | Lil'Log\", 'description': 'Diffusion models have demonstrated strong results on image synthesis in past years. Now the research community has started working on a harder task‚Äîusing it for video generation. The task itself is a superset of the image case, since an image is a video of 1 frame, and it is much more challenging because:\\n\\nIt has extra requirements on temporal consistency across frames in time, which naturally demands more world knowledge to be encoded into the model.\\nIn comparison to text or images, it is more difficult to collect large amounts of high-quality, high-dimensional video data, let along text-video pairs.\\n\\n\\n\\nü•ë Required Pre-read: Please make sure you have read the previous blog on ‚ÄúWhat are Diffusion Models?‚Äù for image generation before continue here.\\n', 'language': 'en'}, page_content='The overview of ControlVideo. (Image source: Zhang et al. 2023)\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Apr 2024). Diffusion Models Video Generation. Lil‚ÄôLog. https://lilianweng.github.io/posts/2024-04-12-diffusion-video/.')], 'answer': 'Agent loops are not directly discussed in the context provided; however, I can provide a general explanation of how they may work and elaborate on the challenges in diffusion video generation based on the given context.\\n\\n**Agent Loops:**\\nAgent loops are a concept often associated with autonomous systems or artificial intelligence where an \"agent\" (a program acting autonomously) continuously performs a cycle of operations. Typically, this involves:\\n1. **Sensing:** Perceiving the environment or input data to gather information.\\n2. **Deciding:** Processing the gathered information to make decisions or predictions using algorithms.\\n3. **Acting:** Taking actions based on the decisions made, which could involve modifying an environment or producing outputs.\\n4. **Learning:** Adapting behaviors or improving the decision-making process based on feedback from the actions taken.\\n\\n**Challenges in Diffusion Video Generation:**\\n1. **Extending Video Length or Increasing Frame Rate:** Diffusion models in video generation need to perform multiple steps of upsampling. This involves sampling additional video frames (e.g., autoregressive extension or interpolating missing frames) conditioned on existing frames to improve the quality and continuity of the video.\\n\\n2. **Sampling Steps and Efficiency:** High sampling rates are computationally intensive. Progressive distillation, like in Imagen Video, reduces the number of sampling steps required, thus speeding up the process significantly without loss in perceptual quality.\\n\\n3. **Complexity Over Image Generation:** Unlike static image generation, video generation involves not just spatial but temporal dynamics, making it inherently more complex. Each frame must relate contextually with others, maintaining coherence over time.\\n\\n4. **Adapting Image Models for Videos:** Many existing models are initially designed for static images. Adapting these models to handle video, which is fundamentally more complex, requires significant innovation and restructuring.\\n\\n5. **Training-Free Adaptation:** Ensuring models can adapt to generate videos without extensive retraining is another challenge. This involves leveraging existing knowledge and model architecture in innovative ways to apply them effectively to video data.\\n\\nThese challenges make video generation a more difficult task than image synthesis, but recent advancements such as progressive distillation illustrate the ongoing progress in the field.'}\n",
      "\n",
      "üîç Sub-questions:\n",
      "- 1. How do agent loops function in the context of diffusion video generation?\n",
      "- 2. What are the common challenges faced when using agent loops for generating diffusion videos?\n",
      "- 3. How do these challenges impact the overall process and quality of diffusion video generation?\n",
      "\n",
      "‚úÖ Final Answer:\n",
      " Agent loops are not directly discussed in the context provided; however, I can provide a general explanation of how they may work and elaborate on the challenges in diffusion video generation based on the given context.\n",
      "\n",
      "**Agent Loops:**\n",
      "Agent loops are a concept often associated with autonomous systems or artificial intelligence where an \"agent\" (a program acting autonomously) continuously performs a cycle of operations. Typically, this involves:\n",
      "1. **Sensing:** Perceiving the environment or input data to gather information.\n",
      "2. **Deciding:** Processing the gathered information to make decisions or predictions using algorithms.\n",
      "3. **Acting:** Taking actions based on the decisions made, which could involve modifying an environment or producing outputs.\n",
      "4. **Learning:** Adapting behaviors or improving the decision-making process based on feedback from the actions taken.\n",
      "\n",
      "**Challenges in Diffusion Video Generation:**\n",
      "1. **Extending Video Length or Increasing Frame Rate:** Diffusion models in video generation need to perform multiple steps of upsampling. This involves sampling additional video frames (e.g., autoregressive extension or interpolating missing frames) conditioned on existing frames to improve the quality and continuity of the video.\n",
      "\n",
      "2. **Sampling Steps and Efficiency:** High sampling rates are computationally intensive. Progressive distillation, like in Imagen Video, reduces the number of sampling steps required, thus speeding up the process significantly without loss in perceptual quality.\n",
      "\n",
      "3. **Complexity Over Image Generation:** Unlike static image generation, video generation involves not just spatial but temporal dynamics, making it inherently more complex. Each frame must relate contextually with others, maintaining coherence over time.\n",
      "\n",
      "4. **Adapting Image Models for Videos:** Many existing models are initially designed for static images. Adapting these models to handle video, which is fundamentally more complex, requires significant innovation and restructuring.\n",
      "\n",
      "5. **Training-Free Adaptation:** Ensuring models can adapt to generate videos without extensive retraining is another challenge. This involves leveraging existing knowledge and model architecture in innovative ways to apply them effectively to video data.\n",
      "\n",
      "These challenges make video generation a more difficult task than image synthesis, but recent advancements such as progressive distillation illustrate the ongoing progress in the field.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 5. Run the pipeline\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"Explain how agent loops work and what are the challenges in diffusion video generation?\"\n",
    "    initial_state = RAGState(question=user_query)\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    print(final_state)\n",
    "\n",
    "    print(\"\\nüîç Sub-questions:\")\n",
    "    for q in final_state['sub_questions']:\n",
    "        print(\"-\", q)\n",
    "\n",
    "    print(\"\\n‚úÖ Final Answer:\\n\", final_state['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
