{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65fcdb54",
   "metadata": {},
   "source": [
    "### üß† What is Query Planning and Decomposition?\n",
    "Query Planning and Decomposition is a technique where a complex user query is broken down into simpler sub-questions or tasks, allowing a system (like a RAG agent) to:\n",
    "\n",
    "- Understand the question more deeply\n",
    "- Retrieve more precise and complete information\n",
    "- Execute step-by-step reasoning\n",
    "\n",
    "It's like reverse-engineering a question into manageable steps before answering.\n",
    "\n",
    "üß† What's New in This Version?\n",
    "- ‚úÖ Add a Query Planner Node\n",
    "- ‚úÖ Break complex user queries into sub-questions\n",
    "- ‚úÖ Retrieve docs per sub-question\n",
    "- ‚úÖ Combine all retrieved contexts\n",
    "- ‚úÖ Generate a final consolidated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc169d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader,WebBaseLoader\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ec6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from dotenv import load_dotenv\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "llm=init_chat_model(\"openai:gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8ed9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1. Load and Embed Documents\n",
    "# ----------------------------\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\"\n",
    "]\n",
    "docs = []\n",
    "for url in urls:\n",
    "    docs.extend(WebBaseLoader(url).load())\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, embedding)\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3a2f524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2. State Schema\n",
    "# ----------------------------\n",
    "class RAGState(BaseModel):\n",
    "    question: str\n",
    "    sub_questions: List[str] = []\n",
    "    retrieved_docs: List[Document] = []\n",
    "    answer: str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d4f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 3. Nodes\n",
    "# ----------------------------\n",
    "\n",
    "## a. Query Planner: splits input question\n",
    "def plan_query(state: RAGState) -> RAGState:\n",
    "   \n",
    "    prompt = f\"\"\"\n",
    "        Break the following complex question into 2-3 sub-questions:\n",
    "\n",
    "        Question: {state.question}\n",
    "\n",
    "        Sub-questions:\n",
    "    \"\"\"\n",
    "    result = llm.invoke(prompt)\n",
    "    sub_questions = [line.strip(\"- \").strip() for line in result.content.strip().split(\"\\n\") if line.strip()]\n",
    "    return RAGState(question=state.question, sub_questions=sub_questions)\n",
    "\n",
    "## b. Retrieve documents for each sub-question\n",
    "def retrieve_for_each(state: RAGState) -> RAGState:\n",
    "    all_docs = []\n",
    "    for sub in state.sub_questions:\n",
    "        docs = retriever.invoke(sub)\n",
    "        all_docs.extend(docs)\n",
    "    return RAGState(question=state.question, sub_questions=state.sub_questions, retrieved_docs=all_docs)\n",
    "\n",
    "## c. Generate final answer\n",
    "def generate_final_answer(state: RAGState) -> RAGState:\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in state.retrieved_docs])\n",
    "    prompt = f\"\"\"\n",
    "        Use the context below to answer the question.\n",
    "\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {state.question}\n",
    "    \"\"\"\n",
    "            \n",
    "    answer = llm.invoke(prompt).content\n",
    "    return RAGState(question=state.question, sub_questions=state.sub_questions, retrieved_docs=state.retrieved_docs, answer=answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26285e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Build LangGraph\n",
    "# ----------------------------\n",
    "builder = StateGraph(RAGState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_query)\n",
    "builder.add_node(\"retriever\", retrieve_for_each)\n",
    "builder.add_node(\"responder\", generate_final_answer)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "builder.add_edge(\"planner\", \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"responder\")\n",
    "builder.add_edge(\"responder\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b7546",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348521ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Run the pipeline\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    user_query = \"Explain how agent loops work and what are the challenges in diffusion video generation?\"\n",
    "    initial_state = RAGState(question=user_query)\n",
    "    final_state = graph.invoke(initial_state)\n",
    "    print(final_state)\n",
    "\n",
    "    print(\"\\nüîç Sub-questions:\")\n",
    "    for q in final_state['sub_questions']:\n",
    "        print(\"-\", q)\n",
    "\n",
    "    print(\"\\n‚úÖ Final Answer:\\n\", final_state['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
