{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52f99f4c",
   "metadata": {},
   "source": [
    "### Hybrid Retriver - Dense and Parse Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "325c0605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b9bf346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain helps build LLM applications.\"),\n",
    "    Document(page_content=\"Pinecone is a vector database for semantic search.\"),\n",
    "    Document(page_content=\"The Eiffel Tower is located in Paris.\"),\n",
    "    Document(page_content=\"Langchain can be used to develop agentic ai application.\"),\n",
    "    Document(page_content=\"Langchain has many types of retrievers.\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9dc4678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Dense Retriver \n",
    "embedding_model = HuggingFaceEmbeddings(model = \"all-MiniLM-L6-v2\")\n",
    "dense_vector_store = FAISS.from_documents(docs, embedding_model)\n",
    "dense_retriver = dense_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99135763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Sparse Retriver (BM25)\n",
    "sparse_retriver = BM25Retriever.from_documents(docs)\n",
    "sparse_retriver.k = 3 # Top k docs to retrive\n",
    "\n",
    "\n",
    "# Step 4 : combine \n",
    "hybrid_retriver = EnsembleRetriever(\n",
    "    retrievers= [dense_retriver, sparse_retriver], \n",
    "    weights= [0.7, 0.3]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1325c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F2F826FC50>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001F2F81ABB60>, k=3)], weights=[0.7, 0.3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54b66e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Document 1:\n",
      "LangChain helps build LLM applications.\n",
      "\n",
      "ðŸ”¹ Document 2:\n",
      "Langchain can be used to develop agentic ai application.\n",
      "\n",
      "ðŸ”¹ Document 3:\n",
      "Langchain has many types of retrievers.\n",
      "\n",
      "ðŸ”¹ Document 4:\n",
      "Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Query and get results\n",
    "query = \"How can I build an application using LLMs?\"\n",
    "results = hybrid_retriver.invoke(query)\n",
    "\n",
    "# Step 6: Print results\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"\\nðŸ”¹ Document {i+1}:\\n{doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f4dc8",
   "metadata": {},
   "source": [
    "### Rag pipeline with Hybrid Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32696758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6c90f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F282AEE490>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F282AEE850>, root_client=<openai.OpenAI object at 0x000001F282AEE210>, root_async_client=<openai.AsyncOpenAI object at 0x000001F282AEE5D0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 5: Prompt Template\n",
    "prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "## step 6-llm\n",
    "llm=init_chat_model(\"openai:gpt-3.5-turbo\",temperature=0.2)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b89488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | EnsembleRetriever(retrievers=[VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F2F826FC50>, search_kwargs={}), BM25Retriever(vectorizer=<rank_bm25.BM25Okapi object at 0x000001F2F81ABB60>, k=3)], weights=[0.7, 0.3]), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\nAnswer the question based on the context below.\\n\\nContext:\\n{context}\\n\\nQuestion: {input}\\n')\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F282AEE490>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F282AEE850>, root_client=<openai.OpenAI object at 0x000001F282AEE210>, root_async_client=<openai.AsyncOpenAI object at 0x000001F282AEE5D0>, temperature=0.2, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create stuff Docuemnt Chain\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=prompt)\n",
    "\n",
    "## create Full rAg chain\n",
    "rag_chain=create_retrieval_chain(retriever=hybrid_retriver,combine_docs_chain=document_chain)\n",
    "rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0e6047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Answer:\n",
      " You can build an app using LLMs by utilizing LangChain, which helps in developing LLM applications. LangChain can be used to develop agentic AI applications, and it offers various types of retrievers to enhance the functionality of your app. Additionally, you can also consider using Pinecone, a vector database for semantic search, to further optimize the performance of your LLM-based app.\n",
      "\n",
      "ðŸ“„ Source Documents:\n",
      "\n",
      "Doc 1: LangChain helps build LLM applications.\n",
      "\n",
      "Doc 2: Langchain can be used to develop agentic ai application.\n",
      "\n",
      "Doc 3: Langchain has many types of retrievers.\n",
      "\n",
      "Doc 4: Pinecone is a vector database for semantic search.\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Ask a question\n",
    "query = {\"input\": \"How can I build an app using LLMs?\"}\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "# Step 10: Output\n",
    "print(\"âœ… Answer:\\n\", response[\"answer\"])\n",
    "\n",
    "print(\"\\nðŸ“„ Source Documents:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nDoc {i+1}: {doc.page_content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
