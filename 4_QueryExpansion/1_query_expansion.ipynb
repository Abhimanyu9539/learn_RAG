{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29bb143",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6eb5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "## step1 : Load and split the dataset\n",
    "loader = TextLoader(\"langchain_crewai_dataset.txt\")\n",
    "raw_docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(raw_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a033b678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0bf7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\RAG\\learn_RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "### step 2: Vector Store\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore=FAISS.from_documents(chunks,embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8394e6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001741EEBFB60>, search_type='mmr', search_kwargs={'k': 5})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 3:MMR Retriever\n",
    "retriever=vectorstore.as_retriever(search_type=\"mmr\",search_kwargs={\"k\":5})\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4f1b2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017444AAD7F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017444AAE270>, root_client=<openai.OpenAI object at 0x0000017444826E40>, root_async_client=<openai.AsyncOpenAI object at 0x0000017444AADFD0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## step 4 : LLM and Prompt\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm=init_chat_model(\"openai:o4-mini\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "236c258d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={}, template='\\nYou are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\\n\\nOriginal query: \"{query}\"\\n\\nExpanded query:\\n')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000017444AAD7F0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017444AAE270>, root_client=<openai.OpenAI object at 0x0000017444826E40>, root_async_client=<openai.AsyncOpenAI object at 0x0000017444AADFD0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query expansion\n",
    "query_expansion_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant. Expand the following query to improve document retrieval by adding relevant synonyms, technical terms, and useful context.\n",
    "\n",
    "Original query: \"{query}\"\n",
    "\n",
    "Expanded query:\n",
    "\"\"\")\n",
    "\n",
    "query_expansion_chain=query_expansion_prompt| llm | StrOutputParser()\n",
    "query_expansion_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb344b85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Expanded query:\\n\\n“LangChain memory” OR “LangChain memory modules” OR “LangChain context persistence” OR “LangChain state management” OR “conversational AI memory in LangChain” OR “chat history storage” OR “session memory”  \\nAND (“ConversationBufferMemory” OR “ConversationSummaryMemory” OR “CombinedMemory” OR “VectorStoreRetrieverMemory” OR “EntityMemory”)  \\nAND (“retrieval-augmented generation (RAG)” OR “embedding-based memory retrieval” OR “LLM chain memory” OR “long-term memory”)  \\nAND (“Python LangChain library” OR “vector database” OR “Chroma” OR “Pinecone” OR “Weaviate” OR “Redis”)  \\nAND (“best practices” OR “architecture” OR “implementation” OR “debugging” OR “performance”)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_expansion_chain.invoke({\"query\":\"Langchain memory\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce2d782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG answering prompt\n",
    "answer_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Answer the question based on the context below.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm=llm,prompt=answer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faa0691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Full RAG pipeline with query expansion\n",
    "rag_pipeline = (\n",
    "    RunnableMap({\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"context\": lambda x: retriever.invoke(query_expansion_chain.invoke({\"query\": x[\"input\"]}))\n",
    "    })\n",
    "    | document_chain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fa9d109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:  \n",
      "“What types of memory (e.g. short-term/session memory, long-term/episodic memory, conversation state, chat history storage) does the LangChain framework support? Please include built-in memory modules and connectors such as in-memory buffers, vector stores (FAISS, Chroma, Pinecone, Weaviate, Milvus), database-backed options (Redis, SQLite, PostgreSQL, MongoDB), file-based persistence, custom memory classes, and any plug-and-play or technical terms used in the LangChain docs for memory management and retrieval.”\n",
      "✅ Answer:\n",
      " LangChain today ships with two out-of-the-box memory abstractions:\n",
      "\n",
      "1. ConversationBufferMemory  \n",
      "   – Simply keeps the raw chat history in memory.\n",
      "\n",
      "2. ConversationSummaryMemory  \n",
      "   – Keeps a running, compressed summary of the dialogue so far (useful once token budgets get tight).\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"What types of memory does LangChain support?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3e0f872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded query:\n",
      "\n",
      "(“CrewAI agents” OR “Crew AI agents” OR “AI crew assistants” OR “autonomous crew agents” OR “intelligent crew management agents” OR “AI‐driven crew coordination” OR “crew scheduling AI” OR “autonomous crew support” OR “crew planning AI platforms”)  \n",
      "AND  \n",
      "(“multi‐agent systems” OR “autonomous agents” OR “agent‐based modeling” OR “distributed AI” OR “reinforcement learning agents” OR “deep learning” OR “natural language processing” OR “knowledge‐based systems” OR “human–AI collaboration”)  \n",
      "AND  \n",
      "(“aviation” OR “maritime” OR “space mission” OR “healthcare” OR “gaming” OR “manufacturing”)  \n",
      "AND  \n",
      "(“features” OR “architecture” OR “use cases” OR “deployment” OR “performance metrics” OR “evaluation” OR “benefits” OR “limitations”)\n",
      "✅ Answer:\n",
      " CrewAI agents are individual, LLM-powered “workers” in a multi-agent orchestration framework. Each agent is given a specific role or responsibility—examples include:  \n",
      "• Researcher: gathers information or data relevant to the task  \n",
      "• Planner: devises a step-by-step approach or high-level strategy  \n",
      "• Executor: carries out actions or generates final content  \n",
      "\n",
      "Key characteristics:  \n",
      "• Semi-independent operation: agents work in parallel but remain aware of shared context  \n",
      "• Structured workflows: roles are chained or orchestrated to ensure tasks flow smoothly from start to finish  \n",
      "• Dynamic communication: agents pass messages, updates, and results among themselves to coordinate and adapt as the task evolves  \n",
      "\n",
      "Together, these agents form a “crew” that divides responsibilities, cooperates on shared objectives, and dynamically adapts to complete complex tasks more efficiently than a single monolithic agent.\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Run query\n",
    "query = {\"input\": \"CrewAI agents?\"}\n",
    "print(query_expansion_chain.invoke({\"query\":query}))\n",
    "response = rag_pipeline.invoke(query)\n",
    "print(\"✅ Answer:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
